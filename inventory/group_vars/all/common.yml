---
### OpenStack control plane configuration

# select OpenStack release: havana, icehouse, juno
openstack_release: juno
use_swift: true
use_ceph: false

search_domain: ".lordbusiness"
nameservers:
 - 172.17.10.22
 - 192.168.70.3
 - 192.168.70.4

### Network interface assignment
# any deviations for individual hosts should be specified in the host_vars or inventory files
primary_if: bond0.1805
private_if: bond0.1801
tunnel_if: bond0.1802

### NTP servers
ntp:
  - 0.rhel.pool.ntp.org
  - 1.rhel.pool.ntp.org
  - 2.rhel.pool.ntp.org

#neutron
neutron_ovs_bridges:
  - bridge: br-ex
    port: bond0.1805
#glance

default_store: swift

### Pacemaker cluster
pcs_cluster_name: automation_cluster
pcs_cluster_pass: "{{ lookup('password', inventory_dir + '/credentials/pcs_cluster_pass chars=ascii_letters,digits') }}"

#database users passwords
clustercheck_db_pass: "{{ lookup('password', inventory_dir + '/credentials/clustercheck_db_pass chars=ascii_letters,digits') }}"
db_root_password: "{{ lookup('password', inventory_dir + '/credentials/db_root_password chars=ascii_letters,digits') }}"
keystone_db_pass: "{{ lookup('password', inventory_dir + '/credentials/keystone_db_pass chars=ascii_letters,digits') }}"
glance_db_pass: "{{ lookup('password', inventory_dir + '/credentials/glance_db_pass chars=ascii_letters,digits') }}"
cinder_db_pass: "{{ lookup('password', inventory_dir + '/credentials/cinder_db_pass chars=ascii_letters,digits') }}"
nova_db_pass: "{{ lookup('password', inventory_dir + '/credentials/nova_db_pass chars=ascii_letters,digits') }}"
neutron_db_pass: "{{ lookup('password', inventory_dir + '/credentials/neutron_db_pass chars=ascii_letters,digits') }}"
heat_db_pass: "{{ lookup('password', inventory_dir + '/credentials/heat_db_pass chars=ascii_letters,digits') }}"
swift_db_pass: "{{ lookup('password', inventory_dir + '/credentials/swift_db_pass chars=ascii_letters,digits') }}"

# passwords for special OpenStack users
admin_pass: "{{ lookup('password', inventory_dir + '/credentials/admin_pass chars=ascii_letters,digits') }}"
glance_pass: "{{ lookup('password', inventory_dir + '/credentials/glance_pass chars=ascii_letters,digits') }}"
cinder_pass: "{{ lookup('password', inventory_dir + '/credentials/cinder_pass chars=ascii_letters,digits') }}"
swift_pass: "{{ lookup('password', inventory_dir + '/credentials/swift_pass chars=ascii_letters,digits') }}"
neutron_pass: "{{ lookup('password', inventory_dir + '/credentials/neutron_pass chars=ascii_letters,digits') }}"
nova_pass: "{{ lookup('password', inventory_dir + '/credentials/nova_pass chars=ascii_letters,digits') }}"
heat_pass: "{{ lookup('password', inventory_dir + '/credentials/heat_pass chars=ascii_letters,digits') }}"
ceilometer_pass: "{{ lookup('password', inventory_dir + '/credentials/ceilometer_pass chars=ascii_letters,digits') }}"

keystone_admin_token: "{{ lookup('password', inventory_dir + '/credentials/keystone_admin_token chars=hexdigits') }}"

### HAproxy parameters
# pacemaker IPaddr2 resources for HAproxy
vip_addresses:
  - name: vip-db
    addr: 172.17.17.101

  - name: vip-msg
    addr: 172.17.17.99

  - name: vip-keystone-int
    addr: 172.17.17.103

  - name: vip-glance-int
    addr: 172.17.17.104

  - name: vip-cinder-int
    addr: 172.17.17.105

  - name: vip-nova-int
    addr: 172.17.17.106

  - name: vip-neutron-int
    addr: 172.17.17.107

  - name: vip-horizon-int
    addr: 172.17.17.108

  - name: vip-heat-int
    addr: 172.17.17.109

  - name: vip-ceilometer-int
    addr: 172.17.17.110

  - name: vip-swift-int
    addr: 172.17.17.111

  - name: vip-scaleio-int
    addr: 172.17.17.112

#  - name: vip-keystone-pub
#    addr: 172.17.20.103

#  - name: vip-glance-pub
#    addr: 172.17.20.104

#  - name: vip-cinder-pub
#    addr: 172.17.20.105

#  - name: vip-nova-pub
#    addr: 172.17.20.106

#  - name: vip-neutron-pub
#    addr: 172.17.20.107

#  - name: vip-horizon-pub
#    addr: 172.17.20.108

#  - name: vip-heat-pub
#    addr: 172.17.20.109

#  - name: vip-ceilometer-pub
#    addr: 172.17.20.110

lb_db_vip: 172.17.17.101
keystone_vip: 172.17.17.103
glance_vip: 172.17.17.104
cinder_vip: 172.17.17.105
nova_vip: 172.17.17.106
neutron_vip: 172.17.17.107
horizon_vip: 172.17.17.108
heat_vip: 172.17.17.109
ceilometer_vip: 172.17.17.110
swift_vip: 172.17.17.111
scaleio_vip: 172.17.17.112

swift_public_vip: "{{ swift_vip }}"
swift_admin_vip: "{{ swift_vip }}"
swift_private_vip: "{{ swift_vip }}"

keystone_public_vip: "{{keystone_vip}}"
keystone_admin_vip: "{{ keystone_vip }}"
keystone_private_vip: "{{ keystone_vip }}"

glance_public_vip: "{{glance_vip}}"
glance_admin_vip: "{{ glance_vip }}"
glance_private_vip: "{{ glance_vip }}"

cinder_public_vip: "{{ cinder_vip }}"
cinder_admin_vip: "{{ cinder_vip }}"
cinder_private_vip: "{{ cinder_vip }}"

nova_public_vip: "{{nova_vip}}"
nova_admin_vip: "{{ nova_vip }}"
nova_private_vip: "{{ nova_vip }}"

neutron_public_vip: "{{ neutron_vip}}"
neutron_admin_vip: "{{ neutron_vip }}"
neutron_private_vip: "{{ neutron_vip }}"

horizon_public_vip: "{{ horizon_vip }}"
horizon_private_vip: "{{ horizon_vip }}"

heat_public_vip: "{{ heat_vip }}"
heat_admin_vip: "{{ heat_vip }}"
heat_private_vip: "{{ heat_vip }}"

ceilometer_public_vip: "{{ ceilometer_vip}}"
ceilometer_admin_vip: "{{ ceilometer_vip }}"
ceilometer_private_vip: "{{ ceilometer_vip }}"


### Horizon
horizon_internal_servername: vip-horizon-int.business
horizon_public_servername: vip-horizon-pub.business
# A url to make first request and get .secret_key_store generated
dashboard_first_request_url: "http://vip-horizon-pub.business/dashboard"





#use_galera: true
#use_cinder_nfs_volume_driver: false
#use_netapp: false
#use_netapp_copyoffload: false
# select Nova Network or Neutron
# use nfs for nova ephemeral volumes
#use_nova_nfs_backend: false
# if use_nova_shared_backend is "true", set nova_shared_volume, nova_shared_fs_type and nova_shared_fs_mount_options
#use_nova_shared_backend: "{{ use_nova_nfs_backend }}"
#use_neutron: true
#### icehouse: select ml2 and its plugins or a standalone Neutron plugin like Cisco Nexus
# use_neutron_ml2: true
# use_neutron_ovs: true
# use_neutron_l3: true
# use_lbaas: true
# use_heat: true
# use_ceilometer: true

### Keystone
# scalability tuning parameters
#keystone_rpc_conn_pool_size: 30
#keystone_rpc_thread_pool_size: 64
#keystone_database_min_pool_size: 1

### Nova
# scalability tuning parameters
#nova_rpc_conn_pool_size: 30
#nova_rpc_thread_pool_size: 64
#nova_database_min_pool_size: 1
#nova_conductor_workers: 4
#nova_osapi_workers: 4

# hypervisor features
#nova_instance_live_migration: true
#nova_network_device_mtu: 9000

# specific for neutron based network implementation
# disable security group
#nova_security_group_api: neutron
#nova_firewall_driver: neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
#nova_firewall_driver: nova.virt.firewall.NoopFirewallDriver
#nova_ovs_bridge_mappings: "physnet1:br1"

# storage related (only use with use_ceph: true)
#nova_libvirt_images_type: raw
#nova_secret_uuid: 522e17af-794c-4db9-95ae-884c48f24b13

# Define the following parameters to use Nova with ephemeral storage on a shared volume
# For example, to use a NFS share:
#nova_shared_volume: "nfs-server.example.com:/srv/nova"
#nova_shared_fs_type: nfs4
#nova_shared_fs_mount_options: "sec=sys,defaults"

# ceilometer related
#nova_notify_on_state_change: vm_and_task_state


### Glance
# default_store: swift
# define the following parameters to use NetApp as a Glance backend
#glance_show_image_direct_url: true
#glance_show_multiple_locations: true
#glance_filesystem_store_metadata_file: /etc/glance/filesystem_store_metadata.json
# this is necessary to support NetApp copy offload
#glance_filesystem_store_file_perm: '0644'

# ceph backend settings
#default_store: rbd
#glance_rbd_user: images
#glance_rbd_pool: images
#rbd_secret_uuid: 522e17af-794c-4db9-95ae-884c48f24b13

# ceph backend settings
#cinder_rbd_user: volumes
#cinder_rbd_pool: volumes

### Ceilometer
#ceilometer_metering_secret: "{{ lookup('password', inventory_dir + '/credentials/ceilometer_metering_secret chars=hexdigits') }}"
# keep last 5 days data only (value is in secs)
#ceilometer_time_to_live: 432000


# #### swift
# # part_power = 2 ^ partition power = partition count. ( The partition is rounded up after calculation.)
# part_power: 16
# #replica couunt =  The number of times that your data will be replicated in the cluster.
# replica_count: 3
# # min_part_hours =  Minimum number of hours before a partition can be moved. This parameter increases availability of data by not moving more than one copy of a given data item within that min_part_hours amount of time.
# min_part_hours: 24
